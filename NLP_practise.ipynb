{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdaSuPTCQGO6MBVEoJvcc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayaKrishanS/NLP_practise/blob/main/NLP_practise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "8iCLlhhr7hiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmgtP7qp_DDn",
        "outputId": "67481543-6886-4fe7-88c3-032524cba94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdqd_uV59Y8K",
        "outputId": "195d94c9-836e-4bf6-9dcd-74faf8c2e6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFRaLfda7RDf"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\" Hi This is Jaya Krishna S, I have completed my masters in Bioinformatics.\n",
        "After that I have done a master data science program with GUVI. Now I'm looking for an oppurtunity to showcase my skillsets.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8LS1HKwp8ff6",
        "outputId": "1226d717-6bc8-447f-b177-4884ff4aaf74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi This is Jaya Krishna S, I have completed my masters in Bioinformatics.\\nAfter that I have done a master data science program with GUVI. Now I'm looking for an oppurtunity to showcase my skillsets.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "mGZgoapY8pBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence tokenization\n",
        "sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC05ymL49B_2",
        "outputId": "a1b19bae-9c57-430d-d309-f5c6e61ba4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Hi This is Jaya Krishna S, I have completed my masters in Bioinformatics.',\n",
              " 'After that I have done a master data science program with GUVI.',\n",
              " \"Now I'm looking for an oppurtunity to showcase my skillsets.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "lHAAXI3G-PAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word tokenization\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "_LzCFgjT-U6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO7kcExW_MCs",
        "outputId": "9f42b394-c121-4749-81f5-e630f0aeca62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'This',\n",
              " 'is',\n",
              " 'Jaya',\n",
              " 'Krishna',\n",
              " 'S',\n",
              " ',',\n",
              " 'I',\n",
              " 'have',\n",
              " 'completed',\n",
              " 'my',\n",
              " 'masters',\n",
              " 'in',\n",
              " 'Bioinformatics',\n",
              " '.',\n",
              " 'After',\n",
              " 'that',\n",
              " 'I',\n",
              " 'have',\n",
              " 'done',\n",
              " 'a',\n",
              " 'master',\n",
              " 'data',\n",
              " 'science',\n",
              " 'program',\n",
              " 'with',\n",
              " 'GUVI',\n",
              " '.',\n",
              " 'Now',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'looking',\n",
              " 'for',\n",
              " 'an',\n",
              " 'oppurtunity',\n",
              " 'to',\n",
              " 'showcase',\n",
              " 'my',\n",
              " 'skillsets',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "C-kEEcBM_tnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "RUrMj6uFA1ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "id": "UJYcRfbhA5Sl",
        "outputId": "087f1c98-04c6-4bd8-d129-fcf9d8dd9480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'This',\n",
              " 'is',\n",
              " 'Jaya',\n",
              " 'Krishna',\n",
              " 'S',\n",
              " ',',\n",
              " 'I',\n",
              " 'have',\n",
              " 'completed',\n",
              " 'my',\n",
              " 'masters',\n",
              " 'in',\n",
              " 'Bioinformatics.',\n",
              " 'After',\n",
              " 'that',\n",
              " 'I',\n",
              " 'have',\n",
              " 'done',\n",
              " 'a',\n",
              " 'master',\n",
              " 'data',\n",
              " 'science',\n",
              " 'program',\n",
              " 'with',\n",
              " 'GUVI.',\n",
              " 'Now',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'looking',\n",
              " 'for',\n",
              " 'an',\n",
              " 'oppurtunity',\n",
              " 'to',\n",
              " 'showcase',\n",
              " 'my',\n",
              " 'skillsets',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vIT2dHdC1U-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming**"
      ],
      "metadata": {
        "id": "Cinh96bZ1Y3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for example we are working on a classification\n",
        "\n",
        "#when solving a problem whether a product has positive or negative review\n",
        "\n",
        "#Reviews ---> ex : eating, eat and eaten ---> all this indicates eat ,,, {going, goes, gone} --> all this indicate go"
      ],
      "metadata": {
        "id": "13_XCm8i1ebd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\", \"eaten\", \"eat\", \"program\", \"programming\", \"History\",\"write\", \"writing\", \"writen\", \"goes\", \"gone\", \"go\"]"
      ],
      "metadata": {
        "id": "hKxx5SBu5IhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Porter stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "bon5xxCx5_hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "jtoxF8ta6b2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in words:\n",
        "  print(i + \"---->\", stemming.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y3Hmzsd6iJV",
        "outputId": "1e256e0e-60af-4da5-fb3c-504033b57aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating----> eat\n",
            "eaten----> eaten\n",
            "eat----> eat\n",
            "program----> program\n",
            "programming----> program\n",
            "History----> histori\n",
            "write----> write\n",
            "writing----> write\n",
            "writen----> writen\n",
            "goes----> goe\n",
            "gone----> gone\n",
            "go----> go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Regexpstemmer\n",
        "\n",
        "from nltk import RegexpStemmer"
      ],
      "metadata": {
        "id": "Vlqsj8xmC_ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer = RegexpStemmer(\"ing$|s$|able$|ed$|en$|en$\", min = 3)"
      ],
      "metadata": {
        "id": "RetBio_JDIYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in words:\n",
        "  print(i, \"--->\" ,reg_stemmer.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qna-SSFyGdSF",
        "outputId": "1b3dad3d-e61b-4d09-8729-448248dbd180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eaten ---> eat\n",
            "eat ---> eat\n",
            "program ---> program\n",
            "programming ---> programm\n",
            "History ---> History\n",
            "write ---> write\n",
            "writing ---> writ\n",
            "writen ---> writ\n",
            "goes ---> goe\n",
            "gone ---> gone\n",
            "go ---> go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "8YqNULNHN73z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snow_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "rSHCejCUOCky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in words:\n",
        "  print(i , \"--->\", snow_stemmer.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBrSVuYaO3yD",
        "outputId": "730d78e0-5ff5-4caf-bbb1-f5a91e008d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eaten ---> eaten\n",
            "eat ---> eat\n",
            "program ---> program\n",
            "programming ---> program\n",
            "History ---> histori\n",
            "write ---> write\n",
            "writing ---> write\n",
            "writen ---> writen\n",
            "goes ---> goe\n",
            "gone ---> gone\n",
            "go ---> go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets see the diffenrent words for porter stemming and snowball stemming\n",
        "\n",
        "print(\"Porter stemming --->\", stemming.stem('fairly'), stemming.stem('clearly'))\n",
        "print(\"Porter stemming --->\", snow_stemmer.stem('fairly'), snow_stemmer.stem('clearly'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjcXTsAzPXTR",
        "outputId": "9931ac8e-1d67-42fe-ac2b-8250981b6954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stemming ---> fairli clearli\n",
            "Porter stemming ---> fair clear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMTCGVa7lx6o",
        "outputId": "8d313a7b-5dc9-467f-814a-d6c7500b8e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "kts9vlQzQRXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "EPHB71_Sk9yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in words:\n",
        "  print(i, \"--->\", lemmatizer.lemmatize(i, pos = 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeP2ppb-lDkX",
        "outputId": "ecbcf0c3-16bb-4f59-86ac-9c70b7571a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eaten ---> eat\n",
            "eat ---> eat\n",
            "program ---> program\n",
            "programming ---> program\n",
            "History ---> History\n",
            "write ---> write\n",
            "writing ---> write\n",
            "writen ---> writen\n",
            "goes ---> go\n",
            "gone ---> go\n",
            "go ---> go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"good\", pos = 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r4tpNDBpnqBO",
        "outputId": "04825b3c-e437-4d20-fef3-7fd2db152fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stop words**"
      ],
      "metadata": {
        "id": "bxUzUlVVspLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "AuKzZMessaFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuqRFoFBs1CF",
        "outputId": "2eb20d88-e856-4418-ec82-411cec12d2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "b4xRPl8yuIIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"In computational linguistics, lemmatization is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighbouring sentences or even an entire document. As a result, developing efficient lemmatization algorithms is an open area of research.\""
      ],
      "metadata": {
        "id": "7SKrx39js589"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para_sentence = sent_tokenize(para)"
      ],
      "metadata": {
        "id": "SIJQjheftUsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-KKvV4ztkAO",
        "outputId": "5b8a0271-0bad-41ae-c6c2-84ebe55c9f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In computational linguistics, lemmatization is the algorithmic process of determining the lemma of a word based on its intended meaning.',\n",
              " 'Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighbouring sentences or even an entire document.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-- > apply stop words\n",
        "\n",
        "for i in len(para_sentence):\n",
        "  stem_words = []\n",
        "  para_tokens = word_tokenize(para_sentence)\n",
        "  for j in para_tokens:\n",
        "    if j not in stopwords:\n",
        "      stem_words.append(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "CnHFL4aktm1d",
        "outputId": "df5ef2c5-8f36-4039-ad58-4eb0437333e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-c90112b90962>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1-- > apply stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mstem_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpara_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cRslkns3Wd-",
        "outputId": "52795102-9436-4d4b-bddb-8eee534c4ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.',\n",
              " 'As a result, developing efficient lemmatization algorithms is an open area of research.']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}